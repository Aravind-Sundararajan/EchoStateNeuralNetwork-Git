{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "import scipy.spatial.distance as ssd\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "# Custom imports\n",
    "from modules import RC_model\n",
    "\n",
    "# Set the colormap for the histogram plot\n",
    "cmap = cm.tab20(np.linspace(0, 1, 12))\n",
    "hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the hyperparameters of the RC model, which define:\n",
    "\n",
    "* the Reservoir\n",
    "* the dimensionality reduction procedure\n",
    "* the MTS representation\n",
    "\n",
    "**Important:** since now we are doing clustering, we set *readout_type = None* and do not specify the other readout hyperparameters. As we will see later, by setting *readout_type = None* the RC model will store the input representations, which will be accessed to compute our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "# Reservoir\n",
    "config['n_internal_units'] = 450        # size of the reservoir\n",
    "config['spectral_radius'] = 0.59        # largest eigenvalue of the reservoir\n",
    "config['leak'] = 0.6                    # amount of leakage in the reservoir state update (None or 1.0 --> no leakage)\n",
    "config['connectivity'] = 0.25           # percentage of nonzero connections in the reservoir\n",
    "config['input_scaling'] = 0.1           # scaling of the input weights\n",
    "config['noise_level'] = 0.01            # noise in the reservoir state update\n",
    "config['n_drop'] = 5                    # transient states to be dropped\n",
    "config['bidir'] = True                  # if True, use bidirectional reservoir\n",
    "config['circ'] = False                  # use reservoir with circle topology\n",
    "\n",
    "# Dimensionality reduction\n",
    "config['dimred_method'] ='tenpca'       # options: {None (no dimensionality reduction), 'pca', 'tenpca'}\n",
    "config['n_dim'] = 75                    # number of resulting dimensions after the dimensionality reduction procedure\n",
    "\n",
    "# MTS representation\n",
    "config['mts_rep'] = 'reservoir'         # MTS representation:  {'last', 'mean', 'output', 'reservoir'}\n",
    "config['w_ridge_embedding'] = 10.0      # regularization parameter of the ridge regression\n",
    "\n",
    "# Readout\n",
    "config['readout_type'] = None           # by setting None, the input representations will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data. How to load the data depends on how your data are stored. In most cases, you will use the Pandas library. In this example, we use Multivariate Time Series (MTS) data stored in a .mat file that can be loaded with the Scipy library.\n",
    "\n",
    "When we do classification, we need a training set to fit the parameters of the RC classifier and a validation/test set to test the classifier performance on new data. Instead, when doing colustering we just look at some structure in the data and we do not need the train/test split.\n",
    "\n",
    "The RC model accepts data represented by a 3-dimensional array of shape $[N,T,V]$, where $N$ is the number of MTS, $T$ is the number of time steps in each MTS, and $V$ is the number of variables in each MTS.\n",
    "\n",
    "**Important:** we assume the number of time steps $T$ to be the same for each of the $N$ MTS. If the MTS have different length, you can use zero-padding or other interpolation techniques to obtain the same length.\n",
    "\n",
    "In this example, we have 640 MTS with 29 time steps and 12 variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/ESNN5np564Cont030421.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/ESNN5np564Cont030421.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-422044618316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ESNN5np564Cont030421'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../dataset/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# shape is [N,T,V]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \"\"\"\n\u001b[0;32m    221\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"generator didn't yield\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python39\\lib\\site-packages\\scipy\\io\\matlab\\mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'.mat'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/ESNN5np564Cont030421.mat'"
     ]
    }
   ],
   "source": [
    "dataset_name = 'ESNN5np564Cont030421'\n",
    "data = scipy.io.loadmat('../dataset/'+dataset_name+'.mat')\n",
    "X = data['X']  # shape is [N,T,V]\n",
    "if len(X.shape) < 3:\n",
    "    X = np.atleast_3d(X)\n",
    "Y = data['Y']  # shape is [N,1]\n",
    "Xte = data['Xte']\n",
    "if len(Xte.shape) < 3:\n",
    "    Xte = np.atleast_3d(Xte)\n",
    "Yte = data['Yte']\n",
    "\n",
    "# Since we are doing clustering, we do not need the train/test split\n",
    "X = np.concatenate((X, Xte), axis=0)\n",
    "Y = np.concatenate((Y, Yte), axis=0)\n",
    "\n",
    "print('Loaded '+dataset_name+' - data shape: '+ str(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our RC model with the hyperparameters specified above.\n",
    "\n",
    "By calling .train(X) we generate the vectorial representations of the MTS contained in X. These representations can be accessed from the .input_repr attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RC model\n",
    "rcm =  RC_model(\n",
    "                reservoir=None,     \n",
    "                n_internal_units=config['n_internal_units'],\n",
    "                spectral_radius=config['spectral_radius'],\n",
    "                leak=config['leak'],\n",
    "                connectivity=config['connectivity'],\n",
    "                input_scaling=config['input_scaling'],\n",
    "                noise_level=config['noise_level'],\n",
    "                circle=config['circ'],\n",
    "                n_drop=config['n_drop'],\n",
    "                bidir=config['bidir'],\n",
    "                dimred_method=config['dimred_method'], \n",
    "                n_dim=config['n_dim'],\n",
    "                mts_rep=config['mts_rep'],\n",
    "                w_ridge_embedding=config['w_ridge_embedding'],\n",
    "                readout_type=config['readout_type'] \n",
    "                )\n",
    "\n",
    "# Generate representations of the input MTS\n",
    "training_time = rcm.train(X)\n",
    "mts_representations = rcm.input_repr\n",
    "print(\"Training time: %.2f seconds\"%training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained the MTS representations we can compute an $N \\times N$ similarity matrix. Since the MTS representations are vectors, we can use any similarity measure for vectors. Here we use the cosine similarity and then normalize the similarity values in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a similarity matrix from the cosine similarity of the representations\n",
    "similarity_matrix = cosine_similarity(mts_representations)\n",
    "        \n",
    "# Normalize the similarity in [0,1]\n",
    "similarity_matrix = (similarity_matrix + 1.0)/2.0\n",
    "\n",
    "# Plot similarity matrix\n",
    "fig =  plt.figure(figsize=(8,8))\n",
    "h = plt.imshow(similarity_matrix)\n",
    "plt.title(\"RC similarity matrix\")\n",
    "plt.colorbar(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check and to see if the similarity matrix makes sense, we can sort the rows and columns according to the class labels. We should now see a block-structure, indicating that MTS of the same class are highly similar to each other and less similar to the MTS of other classes.\n",
    "\n",
    "Note that the classes are imbalanced, i.e., some classes (e.g., class 3) are more populated than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sorted = np.argsort(Y[:,0])\n",
    "similarity_sorted = similarity_matrix[:,idx_sorted][idx_sorted,:]\n",
    "fig =  plt.figure(figsize=(8,8))\n",
    "h = plt.imshow(similarity_sorted)\n",
    "plt.title(\"RC similarity matrix (sorted)\")\n",
    "plt.colorbar(h)\n",
    "class_num, _ = np.histogram(Y[:,0], bins=len(np.unique(Y)))\n",
    "pos = np.cumsum(class_num)\n",
    "plt.xticks(pos, np.unique(Y))\n",
    "plt.yticks(pos, np.unique(Y))\n",
    "plt.xlabel(\"MTS class\")\n",
    "plt.ylabel(\"MTS class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second sanity check, we can see how the MTS look like in two dimensions, according to the similarity matrix we just computed. For this purpose, we will use the Kernel PCA algorithm.\n",
    "\n",
    "We can see that MTS of the same class end up close, confirming that the similarity between MTS in the same class is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction with Kernel PCA\n",
    "kpca = KernelPCA(n_components=2, kernel='precomputed')\n",
    "embeddings_pca = kpca.fit_transform(similarity_matrix)\n",
    "fig =  plt.figure(figsize=(10,8))\n",
    "plt.scatter(embeddings_pca[:,0], embeddings_pca[:,1], c=Y[:,0], s=10, cmap='tab20')\n",
    "plt.title(\"Kernel PCA embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's generate our actual clusters. Most clustering algorithm require a distance (or dissimilarity) matrix. We can compute it from our similarity matrix as follows.\n",
    "\n",
    "Note the diagonal must be zero because the dissimilarity of an element with itself is zero by definition. For small numerical errors, some element in the diagonal might not be exactly zero. Therefore, we force it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Dissimilarity matrix\n",
    "Dist = 1.0 - similarity_matrix\n",
    "np.fill_diagonal(Dist, 0) # due to numerical errors, the diagonal might not be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pletora of clustering algorithm that can be used at this point. The Sklearn library gives you a good range of choices here.\n",
    "\n",
    "Since we know that our MTS are partitioned in 9 classes, we could cheat a bit (note that the class information should not be exploited when doing clustering) and use an algorithm such as k-means with 9 clusters.\n",
    "\n",
    "However, in the following we rather use a hierarchical clustering algorithm that provides us with a nice visualization tool (the dendrogram) that allows us to explore and understand better the structure in our dataset. The number of clusters in the hierarchical algorithm can be obtained by setting a threshold. This threshold should be decided by looking at the dendrogram, so that tall branches are cut. By setting the threshold to t=2.0 we obtain the 9 clusters that we are expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical clustering\n",
    "distArray = ssd.squareform(Dist)\n",
    "Z = linkage(distArray, 'ward')\n",
    "clust = fcluster(Z, t=2.0, criterion=\"distance\")\n",
    "print(\"Found %d clusters\"%len(np.unique(clust)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually there is not a clear way to evaluate the performance of a clustering results. However if class labels are available (like in our case) we can compute the agreement between the class and the cluster labels.\n",
    "\n",
    "There are several metrics to do that. One the is Normalized Mutual Information (which is equivalent to v-score when using a particular averaging function), which evaluates how homogeneous are the class labels of the elements in each cluster. NMI return a value in $[0,1]$ and the higher the better. So, in this case, we are doing pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agreement between class and cluster labels\n",
    "nmi = v_measure_score(Y[:,0], clust)\n",
    "print(\"Normalized Mutual Information (v-score): %.3f\"%nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the dendrogram (this might take a while...) and color the branches according to the threshold 2.0. Indeed, with the threshold 2.0 we are cutting long branches, that indicate there is a clear 9-clusters structure in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "dn = dendrogram(Z, color_threshold=2.0, labels=None, above_threshold_color='k')\n",
    "plt.show()\n",
    "print(\"N. clusters: \", np.unique(dn['color_list']).shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dendrogram, we notice that there is also a clear division in 3 clusters in our dataset. We can obtain such a corarser partition by rising the threshold to 4.5.\n",
    "\n",
    "There is not such a thing as optimal number of clusters and, thus, optimal threshold value. It boils down to the level of resolution you want to look at your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendrogram\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "dn = dendrogram(Z, color_threshold=4.5, labels=None, above_threshold_color='k')\n",
    "plt.show()\n",
    "print(\"N. clusters: \", np.unique(dn['color_list']).shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
